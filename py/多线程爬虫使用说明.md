# 多线程小说爬虫使用说明

## 概述

这个多线程爬虫程序可以同时使用多个线程并发下载小说章节，大大提高了爬取速度。程序会自动分配不同的URL给不同的线程，确保不会重复爬取相同内容。

## 主要特性

- ✅ **多线程并发**: 支持1-20个线程同时工作
- ✅ **智能分配**: 自动将URL分配给不同线程，避免重复爬取
- ✅ **进度显示**: 实时显示爬取进度和线程状态
- ✅ **错误处理**: 完善的异常处理和重试机制
- ✅ **配置灵活**: 可通过配置文件或命令行参数调整设置
- ✅ **统计信息**: 显示总耗时、成功率等统计数据

## 文件说明

- `main.py`: 主程序文件，包含多线程爬取逻辑
- `config.py`: 配置文件，包含URL列表和线程设置
- `run_crawler.py`: 运行脚本，提供多种运行模式
- `novel_chapters/`: 保存下载章节的目录

## 配置说明

在 `config.py` 中可以调整以下设置：

```python
THREAD_CONFIG = {
    'max_workers': 5,        # 最大线程数
    'request_delay': 0.5,    # 每个请求间延时(秒)
    'timeout': 30,           # 请求超时时间(秒)
}
```

### 线程数建议

- **1-3线程**: 安全模式，对服务器压力小，速度较慢
- **5线程**: 推荐设置，平衡速度和稳定性
- **8-10线程**: 快速模式，速度快但对服务器压力较大
- **超过10线程**: 不推荐，可能被服务器限制

## 使用方法

### 方法1：使用运行脚本（推荐）

```bash
python run_crawler.py
```

然后根据提示选择运行模式：
1. 自定义设置 (推荐)
2. 快速模式 (10线程，无延时)
3. 安全模式 (3线程，1秒延时)
4. 直接使用配置文件设置

### 方法2：直接运行主程序

```bash
# 使用默认设置
python main.py

# 使用命令行参数指定线程数
python main.py 8
```

### 方法3：在代码中调用

```python
from main import main, set_thread_count

# 设置线程数
set_thread_count(8)

# 开始爬取
main()
```

## 输出信息说明

程序运行时会显示以下信息：

```
=== 多线程小说内容提取程序开始运行 ===
共找到 1664 个URL需要处理
使用 5 个线程并发处理
请求延时: 0.5 秒
请求超时: 30 秒

[线程ThreadPoolExecutor-0_0] 正在处理第 1/1664 个URL (索引1): https://...
[线程ThreadPoolExecutor-0_1] 正在处理第 2/1664 个URL (索引2): https://...
[线程ThreadPoolExecutor-0_0] 成功保存章节: 第一章 陨落的天才
...

=== 处理完成 ===
成功处理: 1664/1664 个章节
总耗时: 245.67 秒
平均每个章节: 0.15 秒
文件保存在: /path/to/novel_chapters 目录中
```

## 性能对比

以1664个章节为例：

| 模式 | 线程数 | 延时 | 预估总时间 | 优缺点 |
|------|--------|------|------------|--------|
| 单线程 | 1 | 2秒 | ~55分钟 | 稳定但很慢 |
| 安全模式 | 3 | 1秒 | ~18分钟 | 稳定，速度适中 |
| 推荐模式 | 5 | 0.5秒 | ~11分钟 | 平衡速度和稳定性 |
| 快速模式 | 10 | 0秒 | ~5分钟 | 速度快，可能不稳定 |

## 注意事项

1. **合理使用**: 不要设置过多线程，避免对服务器造成过大压力
2. **网络稳定**: 确保网络连接稳定，避免频繁超时
3. **磁盘空间**: 确保有足够磁盘空间保存所有章节
4. **中断恢复**: 程序被中断后重新运行会跳过已存在的文件
5. **错误处理**: 失败的章节会在日志中显示，可以手动重试

## 故障排除

### 常见问题

1. **请求超时**: 增加timeout设置或减少线程数
2. **连接错误**: 检查网络连接，可能需要添加延时
3. **文件重名**: 程序会自动添加序号避免覆盖
4. **内存不足**: 减少线程数

### 优化建议

1. 根据网络状况调整线程数和延时
2. 在网络高峰期使用较少线程
3. 定期检查下载的文件质量
4. 监控程序运行状态，及时处理异常

## 更新日志

- v2.0: 添加多线程支持
- v2.1: 添加配置文件和运行脚本
- v2.2: 优化错误处理和统计显示 